# Import findspark and initialize.
import findspark
findspark.init()


# Import packages
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, concat_ws, lit  
import time

# Create a SparkSession
spark = SparkSession.builder.appName("SparkSQL").getOrCreate()


# 1. Read the home_sales_revised.csv from the provided AWS S3 bucket location into a PySpark DataFrame.
from pyspark import SparkFiles
url = "https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv"

# Add the CSV file to Spark's file system
spark.sparkContext.addFile(url)

# Read the CSV file into a DataFrame
df = spark.read.csv(SparkFiles.get("home_sales_revised.csv"), header=True, inferSchema=True)

# Show the DataFrame to verify the data
df.show(5)



# 2 Ensure date_built is of DateType
# Convert "date_built" to a string representing a valid date (e.g., "2016-01-01")
df = df.withColumn("date_built", concat_ws("-", col("date_built"), lit("01"), lit("01")))

# Cast "date_built" to a proper date format
df = df.withColumn("date_built", col("date_built").cast("date"))

# Show the schema to confirm the conversion
df.printSchema()

# Show the first few rows to confirm the "date_built" transformation
df.select("date_built").show(10)

# 2. Create a temporary view of the DataFrame.
df.createOrReplaceTempView("home_sales")



# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places?
query_3 = """
SELECT YEAR(date) AS year, ROUND(AVG(price), 2) AS avg_price
FROM home_sales
WHERE bedrooms = 4
GROUP BY YEAR(date)
"""
df_3 = spark.sql(query_3)
df_3.show()


# 4. What is the average price of a home for each year the home was built,
# that have 3 bedrooms and 3 bathrooms, rounded to two decimal places?
query_4 = """
SELECT date_built AS year_built, ROUND(AVG(price), 2) AS avg_price
FROM home_sales
WHERE bedrooms = 3 AND bathrooms = 3
GROUP BY date_built
"""
df_4 = spark.sql(query_4)
df_4.show()


# 5. What is the average price of a home for each year the home was built,
# that have 3 bedrooms, 3 bathrooms, with two floors,
# and are greater than or equal to 2,000 square feet, rounded to two decimal places?
query_5 = """
SELECT date_built AS year_built, ROUND(AVG(price), 2) AS avg_price
FROM home_sales
WHERE bedrooms = 3 AND bathrooms = 3 AND floors = 2 AND sqft_living >= 2000
GROUP BY date_built
"""
df_5 = spark.sql(query_5)
df_5.show()


# 6. What is the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000? Order by descending view rating.
query_6 = """
SELECT view, ROUND(AVG(price), 2) AS avg_price
FROM home_sales
GROUP BY view
HAVING avg_price >= 350000
ORDER BY view DESC
"""

# Start the timer
start_time = time.time()

# Run the query and store the result in a DataFrame
df_6 = spark.sql(query_6)
df_6.show()

# Print the time it took to run the query
print("--- %s seconds ---" % (time.time() - start_time))


# 7. Cache the temporary table home_sales.
spark.catalog.cacheTable("home_sales")


# 8. Check if the table is cached.
spark.catalog.isCached('home_sales')


# 9. Using the cached data, run the last query above, that calculates
# the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000.
# Determine the runtime and compare it to the uncached runtime.

start_time = time.time()

# Execute the query again on the cached data
query_6_cached = """
SELECT view, ROUND(AVG(price), 2) AS avg_price
FROM home_sales
GROUP BY view
HAVING avg_price >= 350000
ORDER BY view DESC
"""
df_6_cached = spark.sql(query_6_cached)
df_6_cached.show()

print("--- %s seconds ---" % (time.time() - start_time))


# 10. Partition by the "date_built" field on the formatted parquet home sales data
parquet_path = "home_sales_parquet"

df.write.partitionBy("date_built").parquet(parquet_path, mode="overwrite")


# 11. Read the formatted parquet data.
parquet_df = spark.read.parquet(parquet_path)
# Show the schema to confirm
parquet_df.printSchema()


# 12. Create a temporary table for the parquet data.
parquet_df.createOrReplaceTempView("home_sales_parquet")



# 13. Using the parquet DataFrame, run the last query above, calculate the average price
# of a home per "view" rating, rounded to two decimal places, having an average home price
# greater than or equal to $350,000. Determine the runtime and compare it to the cached runtime.

start_time = time.time()

query_6_parquet = """
SELECT view, ROUND(AVG(price), 2) AS avg_price
FROM home_sales_parquet
GROUP BY view
HAVING avg_price >= 350000
ORDER BY view DESC
"""

# Execute the query on the Parquet data
parquet_df_result = spark.sql(query_6_parquet)
parquet_df_result.show()

# Print runtime
print("--- %s seconds ---" % (time.time() - start_time))


# 14. Uncache the home_sales temporary table.
spark.catalog.uncacheTable("home_sales")


# 15. Check if the home_sales is no longer cached
is_cached = spark.catalog.isCached("home_sales")
print("Is 'home_sales' cached after uncaching?", is_cached)



